## 简介
监控和几乎是每一个生产应用或者服务都需要的东西，在微服务网络中有着成百上千个微服务，如果每个人都要自己去开发或者接入一个监控系统，那这个工作量将会非常大。因此这个工作就由我们基础架构部门统一提供。然后对这个监控系统需要支持以下两个特性：1. 对用户接入是透明的，因为要对接的业务方非常对，每个人都要自己接入，那将是一个很大的工作量；2. 因为是面向所有微服务的，所以这个监控系统拓展性也必须高，也就是它得是分布式的。所以我们在实现的时候是以中间件接入的方式，这样对于用户接入几乎就是透明的，他们只要升级中间件的版本就行了。然后针对拓展性的问题，我们会使用kafka+flink这样的大数据处理栈以及分布式的存储数据库，这样就能解决扩容的问题。这个系统的实现理论上主要按照tracing、metrics、logging这三大模块来设计。tacing主要是用来分析链路性能瓶颈和梳理服务拓扑关系，metrics主要就是指标和实时告警，logging主要是日志排查是事件收集。系统的数据源也根据这个分为三块，其中指标数据都是直接发送kafka，链路数据是发生到一个agent再发生到kafka，日志数据是打印在本地然后通过logagent收集到kafka，然后通过flink对kafka中的数据进行过滤和加工最后分别存储到对应的存储中。指标数据最后存储在opentsdb中，链路数据和日志数据都存储在es中。最后用户通过统一的web门面来查询需要的信息。

## 难点
* 采集的数据需要进行合理的设计，如果没设计好，可能导致数据量会随着业务增长而急剧增长，带来的机器成本也急剧增长，同时对业务的吞吐量也会有一定影响
* 对大数据处理有一定的技术要求，进入kafka的数据不是直接丢入后端的存储，还要用来做很多事，比如告警，拓扑分析，故障根因分析等
* 系统要有自己监控的能力，我们的系统如果发生故障，那么业务人员对线上的微服务就会失去监控，就不能及时发现故障和解决，所以我们要能及时发现自己系统的问题和及时恢复

## 遇到的问题及解决方案
* 有一天我们突然出现所有指标都延时了，就是当前只能看到几十分钟前的数据，并且所有指标都是这样，我们就怀疑tsdb那边是不是有问题了，再看看指标写入的job发现tsdb写入的速度非常慢产生了积压，然后一查最近的发布事件发现刚发了个新服务，然后就去查和这个服务有关的指标信息，发现这个服务的指标数量非常多，多大了十几万个指标，最后排查发现是因为他们的指标用了用户id这样的tag，这种tag是完全动态，在指标聚合上是没有意义，还会让指标的数量膨胀得很大。然后我们一方面需要通知业务方改进，一方面要自己先在系统中解决这个问题。所以我们就在指标处理的job先做一个统计，如果一个指标的tag超过了一定数量，我们会把它丢掉并且把这个记录下来，通知业务方改进，等业务方改好后再把这个指标从黑名单中放出来。
* 链路数据是采样收集的，对那种调用量大的链路这是没有问题的，但是对那种调用量小的链路是很不友好的，因为采集的样本很少，像少量的错误或者慢调用就可能采集不到。所以我们就得优化一下采样策略，一开始我们提供了用户配置的方式，但是后面发现每个都要配置一下用户就不乐意了，我们就考虑自适应的方式。在考虑自适应的方案时有两种实现方式，一种是在应用内部做记录，一种是在外部做统计。如果是在内部做记录的话一是会给应用带来的压力，尤其是网关，因为几乎所有的链路都是从网关发起的，那网关的统计任务将非常大，而且不方便更新。所以我们决定在外部做这个统计，然后生成合适的采样率再下发到服务中去。
* 
## 改进
* 架构优化，指标数据现在是直接发送到kafka的，这样服务就会对kafka产生了一个直接的依赖，用户在上线服务的时候还得知道对应的kafka集群信息，以后kafka集群需要变动就会非常麻烦，所以我想把指标信息像链路信息一样通过udp发送到本地的一个agent，然后由agent发送到kafka，这样后面kafka出问题了，也只会影响到agent，并不会影响到服务本身
* 用AI来解决问题，像告警阈值配置每个服务都要自己配一下也是挺痛苦的，而且用户有时也不确定应该配多少合适，未来可以尝试通过AI来帮解决这个问题，既能减少他们的工作量还能给他们合理的建议
* 交互优化，因为页面比较多，很多用户反应交互不是很理想，有时出了问题不知道应该看哪里，所以需要在交互上进行更好的设计，让用户能傻瓜似的快速解决自己的问题，不能让复杂的交互增加他们的学习成本